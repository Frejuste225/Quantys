import os
import json
from flask import Flask, request, jsonify, send_file
from flask_cors import CORS
import pandas as pd
from datetime import datetime, date
import uuid
from werkzeug.utils import secure_filename
import logging
from dotenv import load_dotenv
import re

# Charger les variables d'environnement
load_dotenv()

# Imports des services
from services.session_service import SessionService
from services.file_processor import FileProcessorService
from services.file_manager import FileManager
from services.lotecart_processor import LotecartProcessor
from utils.validators import FileValidator
from utils.error_handler import APIErrorHandler, handle_api_errors
from utils.rate_limiter import apply_rate_limit
from database import db_manager

app = Flask(__name__)
CORS(app, expose_headers=["Content-Disposition"])


# Configuration am√©lior√©e
class Config:
    def __init__(self):
        self.UPLOAD_FOLDER = os.getenv("UPLOAD_FOLDER", "uploads")
        self.PROCESSED_FOLDER = os.getenv("PROCESSED_FOLDER", "processed")
        self.FINAL_FOLDER = os.getenv("FINAL_FOLDER", "final")
        self.ARCHIVE_FOLDER = os.getenv("ARCHIVE_FOLDER", "archive")
        self.LOG_FOLDER = os.getenv("LOG_FOLDER", "logs")
        self.MAX_FILE_SIZE = int(os.getenv("MAX_FILE_SIZE", 16 * 1024 * 1024))
        self.SECRET_KEY = os.getenv("SECRET_KEY", "dev-key-change-in-production")

        # Cr√©er les r√©pertoires
        for folder in [
            self.UPLOAD_FOLDER,
            self.PROCESSED_FOLDER,
            self.FINAL_FOLDER,
            self.ARCHIVE_FOLDER,
            self.LOG_FOLDER,
        ]:
            os.makedirs(folder, exist_ok=True)


config = Config()
app.config.from_object(config)
app.secret_key = config.SECRET_KEY

# Configuration du logging am√©lior√©e
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(os.path.join(config.LOG_FOLDER, "inventory_processor.log")),
        logging.StreamHandler(),
    ],
)
logger = logging.getLogger(__name__)

# Initialisation des services
session_service = SessionService()
file_processor = FileProcessorService()
file_manager = FileManager(
    {
        "UPLOAD_FOLDER": config.UPLOAD_FOLDER,
        "PROCESSED_FOLDER": config.PROCESSED_FOLDER,
        "FINAL_FOLDER": config.FINAL_FOLDER,
        "ARCHIVE_FOLDER": config.ARCHIVE_FOLDER,
    }
)


# Classe de compatibilit√© (pour migration progressive)
class SageX3Processor:
    """
    Classe de compatibilit√© - utilise maintenant les services
    """

    def __init__(self):
        self.session_service = session_service
        self.file_processor = file_processor
        # Initialiser le processeur LOTECART
        from services.lotecart_processor import LotecartProcessor
        self.lotecart_processor = LotecartProcessor()
        self.lotecart_processor = LotecartProcessor()

    def process_completed_file(self, session_id: str, completed_file_path: str):
        """Traite le fichier Excel compl√©t√© et calcule les √©carts"""
        try:
            # Lire le fichier Excel compl√©t√©
            completed_df = pd.read_excel(completed_file_path)

            # Validation des colonnes requises
            required_columns = [
                "Code Article",
                "Quantit√© Th√©orique",
                "Quantit√© R√©elle",
                "Num√©ro Lot",
                "Num√©ro Inventaire",
            ]
            missing_columns = [
                col for col in required_columns if col not in completed_df.columns
            ]
            if missing_columns:
                raise ValueError(
                    f"Colonnes manquantes dans le fichier: {', '.join(missing_columns)}"
                )

            # Nettoyer les num√©ros de lot dans le fichier compl√©t√©
            completed_df["Num√©ro Lot"] = (
                completed_df["Num√©ro Lot"].fillna("").astype(str).str.strip()
            )
            completed_df.loc[
                completed_df["Num√©ro Lot"].str.upper().isin(["NAN", "NULL", "NONE"]),
                "Num√©ro Lot",
            ] = ""

            # Conversion des types
            completed_df["Quantit√© Th√©orique"] = pd.to_numeric(
                completed_df["Quantit√© Th√©orique"], errors="coerce"
            )
            completed_df["Quantit√© R√©elle"] = pd.to_numeric(
                completed_df["Quantit√© R√©elle"], errors="coerce"
            )

            # Calcul des √©carts
            completed_df["√âcart"] = (
                completed_df["Quantit√© R√©elle"] - completed_df["Quantit√© Th√©orique"]
            )

            # D√©tection et traitement des lots LOTECART avec le processeur sp√©cialis√©
            lotecart_candidates = self.lotecart_processor.detect_lotecart_candidates(completed_df)
            
            # Marquer les lignes LOTECART dans le DataFrame principal
            if not lotecart_candidates.empty:
                lotecart_mask = (completed_df["Quantit√© Th√©orique"] == 0) & (
                    completed_df["Quantit√© R√©elle"] > 0
                )
                completed_df.loc[lotecart_mask, "Type_Lot"] = "lotecart"
                
                # Sauvegarder les candidats LOTECART pour traitement ult√©rieur
                self.session_service.save_dataframe(session_id, "lotecart_candidates", lotecart_candidates)

            # Filtrer les articles avec √©carts
            discrepancies_df = completed_df[completed_df["√âcart"] != 0].copy()

            # Statistiques
            total_discrepancy = float(discrepancies_df["√âcart"].sum())
            adjusted_items_count = len(discrepancies_df)

            # Sauvegarder les r√©sultats dans les services
            self.session_service.save_dataframe(session_id, "completed_df", completed_df)
            self.session_service.save_dataframe(session_id, "discrepancies_df", discrepancies_df)

            # Mettre √† jour la session en base
            self.session_service.update_session(
                session_id,
                total_discrepancy=total_discrepancy,
                adjusted_items_count=adjusted_items_count,
            )

            logger.info(
                f"Fichier compl√©t√© trait√© pour session {session_id}: {adjusted_items_count} lots avec √©carts"
            )
            return discrepancies_df

        except Exception as e:
            logger.error(f"Erreur traitement fichier compl√©t√©: {e}")
            raise

    def distribute_discrepancies(self, session_id: str, strategy: str = "FIFO"):
        """Distribue les √©carts selon la strat√©gie choisie avec priorit√© sur les types de lots"""
        try:
            # Charger les donn√©es depuis les services
            discrepancies_df = self.session_service.load_dataframe(session_id, "discrepancies_df")
            original_df = self.session_service.load_dataframe(session_id, "original_df")
            
            if discrepancies_df is None or original_df is None:
                raise ValueError("Donn√©es de session manquantes pour la distribution")

            # Cr√©er une liste pour stocker les ajustements
            adjustments = []

            for _, discrepancy_row in discrepancies_df.iterrows():
                code_article = discrepancy_row["Code Article"]
                numero_inventaire = discrepancy_row.get("Num√©ro Inventaire", "")
                ecart = discrepancy_row["√âcart"]

                if ecart == 0:
                    continue

                # V√©rifier si c'est un cas LOTECART dans les √©carts
                is_lotecart = discrepancy_row.get("Type_Lot") == "lotecart"

                if is_lotecart:
                    # Traitement LOTECART avec le processeur sp√©cialis√©
                    logger.info(
                        f"üéØ Lot LOTECART d√©tect√© pour {code_article} - "
                        f"Quantit√© th√©orique: 0, Quantit√© r√©elle: {discrepancy_row.get('Quantit√© R√©elle', 0)}"
                    )
                    
                    # Cr√©er un DataFrame temporaire pour ce candidat LOTECART
                    lotecart_candidate = pd.DataFrame([discrepancy_row])
                    
                    # Utiliser le processeur LOTECART pour cr√©er les ajustements
                    lotecart_adjustments = self.lotecart_processor.create_lotecart_adjustments(
                        lotecart_candidate, original_df
                    )
                    
                    # Ajouter les ajustements LOTECART √† la liste principale
                    adjustments.extend(lotecart_adjustments)
                    
                    logger.info(f"‚úÖ {len(lotecart_adjustments)} ajustements LOTECART cr√©√©s pour {code_article}")
                    continue

                # Traitement normal pour les autres types de lots
                # Trouver tous les lots pour cet article et cet inventaire
                if numero_inventaire:
                    article_lots = original_df[
                        (original_df["CODE_ARTICLE"] == code_article)
                        & (original_df["NUMERO_INVENTAIRE"] == numero_inventaire)
                    ].copy()
                else:
                    article_lots = original_df[
                        original_df["CODE_ARTICLE"] == code_article
                    ].copy()

                if article_lots.empty:
                    continue

                article_lots = self._sort_lots_by_priority_and_strategy(
                    article_lots, strategy
                )

                # Distribuer l'√©cart
                remaining_discrepancy = ecart

                for _, lot_row in article_lots.iterrows():
                    if (
                        abs(remaining_discrepancy) < 0.001
                    ):  # √âviter les erreurs de pr√©cision
                        break

                    lot_quantity = float(lot_row["QUANTITE"])
                    lot_number = lot_row["NUMERO_LOT"] if lot_row["NUMERO_LOT"] else ""

                    if remaining_discrepancy > 0:
                        # √âcart positif : ajouter du stock
                        adjustment = min(
                            remaining_discrepancy, lot_quantity * 2
                        )  # Limite arbitraire
                    else:
                        # √âcart n√©gatif : retirer du stock
                        adjustment = max(remaining_discrepancy, -lot_quantity)

                    if abs(adjustment) > 0.001:
                        adjustments.append(
                            {
                                "CODE_ARTICLE": code_article,
                                "NUMERO_INVENTAIRE": numero_inventaire,
                                "NUMERO_LOT": lot_number,
                                "TYPE_LOT": lot_row.get("Type_Lot", "unknown"),
                                "QUANTITE_ORIGINALE": lot_quantity,
                                "AJUSTEMENT": adjustment,
                                "QUANTITE_CORRIGEE": lot_quantity + adjustment,
                                "Date_Lot": lot_row["Date_Lot"],
                                "original_s_line_raw": lot_row["original_s_line_raw"],
                            }
                        )

                        remaining_discrepancy -= adjustment

            # Convertir en DataFrame
            distributed_df = pd.DataFrame(adjustments)

            # Sauvegarder dans les services
            self.session_service.save_dataframe(session_id, "distributed_df", distributed_df)

            logger.info(
                f"√âcarts distribu√©s pour session {session_id} avec strat√©gie {strategy}: {len(adjustments)} ajustements"
            )
            return distributed_df

        except Exception as e:
            logger.error(f"Erreur distribution √©carts: {e}")
            raise

    def _sort_lots_by_priority_and_strategy(
        self, lots_df: pd.DataFrame, strategy: str
    ) -> pd.DataFrame:
        """Trie les lots selon la priorit√© des types et la strat√©gie FIFO/LIFO"""
        # D√©finir l'ordre de priorit√© des types de lots (simplifi√©)
        type_priority = {"type1": 1, "type2": 2, "lotecart": 3, "unknown": 4}

        # Ajouter une colonne de priorit√©
        lots_df["priority"] = (
            lots_df.get("Type_Lot", "unknown").map(type_priority).fillna(4)
        )

        # Trier d'abord par priorit√© de type, puis par date selon la strat√©gie
        if strategy == "FIFO":
            # Type prioritaire d'abord, puis plus anciens d'abord
            sorted_lots = lots_df.sort_values(
                ["priority", "Date_Lot"], na_position="last"
            )
        else:  # LIFO
            # Type prioritaire d'abord, puis plus r√©cents d'abord
            sorted_lots = lots_df.sort_values(
                ["priority", "Date_Lot"], ascending=[True, False], na_position="last"
            )

        # Pour les lots LOTECART, on ignore la date et on prend le premier disponible
        lotecart_lots = sorted_lots[sorted_lots.get("Type_Lot", "") == "lotecart"]
        other_lots = sorted_lots[sorted_lots.get("Type_Lot", "") != "lotecart"]

        # Recombiner : autres lots tri√©s + lots LOTECART en premier disponible
        if not lotecart_lots.empty:
            result = pd.concat([other_lots, lotecart_lots], ignore_index=True)
        else:
            result = other_lots

        return result.drop("priority", axis=1, errors="ignore")

    def generate_final_file(self, session_id: str):
        """G√©n√®re le fichier CSV final au format Sage X3 avec TOUTES les lignes originales"""
        try:
            # Charger les donn√©es depuis les services
            distributed_df = self.session_service.load_dataframe(session_id, "distributed_df")
            original_df = self.session_service.load_dataframe(session_id, "original_df")
            completed_df = self.session_service.load_dataframe(session_id, "completed_df")
            
            if distributed_df is None or original_df is None or completed_df is None:
                raise ValueError("Donn√©es manquantes pour g√©n√©rer le fichier final")

            # R√©cup√©rer les donn√©es de session depuis la base
            db_session_data = self.session_service.get_session_data(session_id)
            if not db_session_data:
                raise ValueError("Session non trouv√©e en base")
            
            # R√©cup√©rer les header_lines depuis la base
            import json
            header_lines = json.loads(db_session_data["header_lines"]) if db_session_data["header_lines"] else []

            # Construire le nom du fichier
            original_filename = db_session_data["original_filename"]
            base_name = os.path.splitext(original_filename)[0]
            final_filename = f"{base_name}_corrige_{session_id}.csv"
            final_file_path = os.path.join(config.FINAL_FOLDER, final_filename)

            # Cr√©er un dictionnaire des quantit√©s r√©elles depuis le template compl√©t√©
            # Cl√©: (CODE_ARTICLE, NUMERO_INVENTAIRE, NUMERO_LOT)
            real_quantities_dict = {}
            for _, row in completed_df.iterrows():
                code_article = row["Code Article"]
                numero_inventaire = row["Num√©ro Inventaire"]
                numero_lot = str(row["Num√©ro Lot"]).strip() if pd.notna(row["Num√©ro Lot"]) else ""
                quantite_reelle = row["Quantit√© R√©elle"]
                
                key = (code_article, numero_inventaire, numero_lot)
                real_quantities_dict[key] = quantite_reelle
            
            # Cr√©er un dictionnaire des ajustements pour un acc√®s rapide
            adjustments_dict = {}
            for _, row in distributed_df.iterrows():
                code_article = row["CODE_ARTICLE"]
                numero_inventaire = row["NUMERO_INVENTAIRE"]
                numero_lot = (
                    str(row["NUMERO_LOT"]).strip()
                    if pd.notna(row["NUMERO_LOT"])
                    else ""
                )

                key = (code_article, numero_inventaire, numero_lot)
                adjustments_dict[key] = {
                    "QUANTITE_CORRIGEE": row["QUANTITE_CORRIGEE"],
                    "TYPE_LOT": row["TYPE_LOT"],
                    "AJUSTEMENT": row["AJUSTEMENT"],
                    "IS_NEW_LOTECART": pd.isna(row.get("original_s_line_raw")) or row.get("original_s_line_raw") is None
                }

            # G√©n√©rer le contenu du fichier
            lines = []

            # Ajouter les en-t√™tes E et L
            lines.extend(header_lines)

            # Traiter TOUTES les lignes originales
            lines_processed = 0
            lines_adjusted = 0

            for _, original_row in original_df.iterrows():
                if pd.notna(original_row["original_s_line_raw"]):
                    original_line = str(original_row["original_s_line_raw"])
                    parts = original_line.split(";")

                    if len(parts) >= 6:  # S'assurer qu'on a assez de colonnes
                        # Cr√©er la cl√© pour chercher un ajustement
                        code_article = original_row["CODE_ARTICLE"]
                        numero_inventaire = original_row["NUMERO_INVENTAIRE"]
                        numero_lot = (
                            str(original_row["NUMERO_LOT"]).strip()
                            if pd.notna(original_row["NUMERO_LOT"])
                            else ""
                        )

                        key = (code_article, numero_inventaire, numero_lot)
                        
                        # R√©cup√©rer la quantit√© r√©elle depuis le template compl√©t√©
                        quantite_reelle = real_quantities_dict.get(key, 0)

                        # üéØ LOGIQUE AM√âLIOR√âE : Quantit√©s th√©oriques ajust√©es + quantit√©s r√©elles saisies
                        
                        # 1. TOUJOURS mettre √† jour la quantit√© r√©elle depuis le template compl√©t√©
                        if quantite_reelle is not None:
                            parts[6] = str(int(quantite_reelle))  # Colonne 6 = Quantit√© r√©elle saisie
                        
                        # 2. V√©rifier s'il y a un ajustement pour cette ligne
                        if key in adjustments_dict:
                            adjustment = adjustments_dict[key]
                            
                            # 3. Appliquer l'ajustement sur la quantit√© th√©orique (colonne 5)
                            if adjustment["TYPE_LOT"] == "lotecart":
                                # Pour LOTECART : quantit√© th√©orique = quantit√© r√©elle (pas d'√©cart)
                                parts[5] = str(int(quantite_reelle))
                                logger.debug(f"üè∑Ô∏è LOTECART {code_article}: Qt√© th√©o = Qt√© r√©elle = {quantite_reelle}")
                            else:
                                # Pour ajustements normaux : quantit√© th√©orique ajust√©e
                                parts[5] = str(int(adjustment["QUANTITE_CORRIGEE"]))
                                logger.debug(f"‚öñÔ∏è Ajustement {code_article}: Qt√© th√©o ajust√©e = {adjustment['QUANTITE_CORRIGEE']}, Qt√© r√©elle = {quantite_reelle}")

                            # 4. S'assurer que le num√©ro de lot est correct
                            if len(parts) > 14:
                                if adjustment["TYPE_LOT"] == "lotecart" or numero_lot == "LOTECART":
                                    parts[14] = "LOTECART"
                                else:
                                    parts[14] = numero_lot

                            lines_adjusted += 1
                            
                        else:
                            # 5. Pour les lignes NON ajust√©es : garder quantit√© th√©orique originale
                            # La quantit√© r√©elle a d√©j√† √©t√© mise √† jour ci-dessus
                            quantite_theo_originale = parts[5]
                            logger.debug(f"üìã Ligne standard {code_article}: Qt√© th√©o originale = {quantite_theo_originale}, Qt√© r√©elle saisie = {quantite_reelle}")
                        
                        # 6. Log de v√©rification pour tra√ßabilit√©
                        final_qte_theo = parts[5]
                        final_qte_reelle = parts[6]
                        ecart_final = float(final_qte_reelle) - float(final_qte_theo) if final_qte_theo and final_qte_reelle else 0
                        
                        if abs(ecart_final) > 0.001:  # Il y a encore un √©cart
                            logger.debug(f"üìä {code_article} - √âcart final: {ecart_final} (Th√©o: {final_qte_theo}, R√©el: {final_qte_reelle})")
                        else:
                            logger.debug(f"‚úÖ {code_article} - Pas d'√©cart (Th√©o: {final_qte_theo}, R√©el: {final_qte_reelle})")

                        # V√©rifier si la quantit√© finale est nulle et mettre INDICATEUR_COMPTE √† 2
                        quantite_finale = float(parts[5]) if parts[5] else 0
                        quantite_theorique_originale = float(
                            original_row.get("QUANTITE", 0)
                        )
                        quantite_reelle_finale = float(parts[6]) if parts[6] else 0

                        # Mettre INDICATEUR_COMPTE √† 2 dans les cas suivants :
                        # 1. La quantit√© th√©orique finale est 0 ET quantit√© r√©elle > 0 (LOTECART)
                        # 2. La quantit√© th√©orique originale √©tait 0 (cas LOTECART d√©tect√©)
                        # 3. Les quantit√©s th√©orique et r√©elle sont √©gales (pas d'√©cart)
                        if (
                            (quantite_theorique_originale == 0 and quantite_reelle_finale > 0) or
                            (quantite_finale == quantite_reelle_finale and quantite_reelle_finale > 0) or
                            numero_lot == "LOTECART"
                        ) and len(parts) > 7:
                            parts[7] = "2"  # INDICATEUR_COMPTE √† l'index 7
                            logger.debug(
                                f"INDICATEUR_COMPTE mis √† 2 pour {code_article} - {numero_lot} (qt√© th√©o finale: {quantite_finale}, qt√© r√©elle: {quantite_reelle_finale})"
                            )

                        # Ajouter la ligne (ajust√©e ou originale)
                        corrected_line = ";".join(parts)
                        lines.append(corrected_line)
                        lines_processed += 1

            # G√©n√©rer les nouvelles lignes LOTECART avec le processeur sp√©cialis√©
            max_line_number = 0
            if original_df is not None and not original_df.empty:
                # Extraire les num√©ros de ligne existants pour √©viter les doublons
                line_numbers = []
                for _, row in original_df.iterrows():
                    line_raw = str(row.get("original_s_line_raw", ""))
                    parts = line_raw.split(";")
                    if len(parts) > 3:
                        try:
                            line_num = int(parts[3])
                            line_numbers.append(line_num)
                        except (ValueError, IndexError):
                            pass
                max_line_number = max(line_numbers) if line_numbers else 0

            # Filtrer les ajustements LOTECART qui n√©cessitent de nouvelles lignes
            lotecart_adjustments = [
                adj for _, adj in distributed_df.iterrows()
                if (adj.get("TYPE_LOT") == "lotecart" and 
                    (pd.isna(adj.get("original_s_line_raw")) or adj.get("original_s_line_raw") is None))
            ]
            
            # Convertir en format attendu par le processeur LOTECART
            lotecart_adjustments_dict = []
            for adj in lotecart_adjustments:
                # R√©cup√©rer la quantit√© r√©elle depuis le template compl√©t√©
                key = (adj["CODE_ARTICLE"], adj["NUMERO_INVENTAIRE"], "LOTECART")
                quantite_reelle = real_quantities_dict.get(key, adj["QUANTITE_CORRIGEE"])
                
                lotecart_adjustments_dict.append({
                    "CODE_ARTICLE": adj["CODE_ARTICLE"],
                    "NUMERO_INVENTAIRE": adj["NUMERO_INVENTAIRE"],
                    "NUMERO_LOT": "LOTECART",
                    "TYPE_LOT": "lotecart",
                    "QUANTITE_CORRIGEE": adj["QUANTITE_CORRIGEE"],
                    "QUANTITE_REELLE": quantite_reelle,  # Ajouter la quantit√© r√©elle
                    "reference_line": adj.get("reference_line"),
                    "is_new_lotecart": True
                })

            # G√©n√©rer les nouvelles lignes LOTECART
            new_lotecart_lines = self.lotecart_processor.generate_lotecart_lines(
                lotecart_adjustments_dict, max_line_number
            )
            
            # Ajouter les nouvelles lignes au fichier
            lines.extend(new_lotecart_lines)
            lotecart_lines_created = len(new_lotecart_lines)
            
            logger.info(f"üéØ {lotecart_lines_created} nouvelles lignes LOTECART ajout√©es au fichier final")

            # √âcrire le fichier
            with open(final_file_path, "w", encoding="utf-8", newline="") as f:
                for line in lines:
                    f.write(line + "\n")

            # Mettre √† jour la session
            self.session_service.update_session(
                session_id, final_file_path=final_file_path
            )

            logger.info(f"Fichier final g√©n√©r√©: {final_file_path}")
            logger.info(
                f"Total lignes trait√©es: {lines_processed}, Lignes ajust√©es: {lines_adjusted}, Nouvelles lignes LOTECART: {lotecart_lines_created}"
            )
            
            # V√©rification finale avec le processeur LOTECART
            expected_lotecart_count = lotecart_lines_created
            validation_result = self.lotecart_processor.validate_lotecart_processing(
                final_file_path, expected_lotecart_count
            )
            
            if validation_result["success"]:
                logger.info("‚úÖ Validation LOTECART r√©ussie")
            else:
                logger.warning(f"‚ö†Ô∏è Probl√®mes d√©tect√©s lors de la validation LOTECART: {validation_result['issues']}")
            
            # V√©rification finale g√©n√©rale avec r√©sum√© d√©taill√©
            summary = self._verify_final_file_with_summary(final_file_path)
            logger.info(f"üìä R√©sum√© du fichier final: {summary}")
            
            # V√©rification sp√©cifique des quantit√©s th√©oriques ajust√©es vs quantit√©s r√©elles
            quantities_verification = self._verify_quantities_consistency(
                final_file_path, completed_df, distributed_df
            )
            logger.info(f"üîç V√©rification quantit√©s: {quantities_verification}")
            
            return final_file_path

        except Exception as e:
            logger.error(f"Erreur g√©n√©ration fichier final: {e}")
            raise
    
    def _verify_final_file_with_summary(self, final_file_path: str) -> dict:
        """V√©rifie le contenu du fichier final g√©n√©r√© et retourne un r√©sum√© d√©taill√©"""
        summary = {
            "success": True,
            "file_path": final_file_path,
            "verification_timestamp": datetime.now().isoformat(),
            "structure": {
                "total_lines": 0,
                "header_lines_e": 0,
                "header_lines_l": 0,
                "data_lines_s": 0,
                "invalid_lines": 0
            },
            "quantities": {
                "total_theoretical": 0.0,
                "total_real": 0.0,
                "total_discrepancy": 0.0,
                "zero_theoretical_count": 0,
                "zero_real_count": 0,
                "negative_quantities": 0
            },
            "lotecart": {
                "total_lines": 0,
                "correct_indicators": 0,
                "incorrect_indicators": 0,
                "sample_articles": []
            },
            "adjustments": {
                "lines_with_adjustments": 0,
                "lines_with_indicator_2": 0,
                "articles_adjusted": set(),
                "adjustment_summary": {}
            },
            "quality": {
                "lines_with_missing_data": 0,
                "lines_with_invalid_quantities": 0,
                "duplicate_line_numbers": [],
                "inconsistent_inventories": []
            },
            "sample_data": {
                "first_10_s_lines": [],
                "lotecart_samples": [],
                "adjustment_samples": []
            },
            "issues": [],
            "warnings": []
        }
        
        try:
            if not os.path.exists(final_file_path):
                summary["success"] = False
                summary["issues"].append("Fichier final non trouv√©")
                return summary
            
            # Dictionnaires pour tracking
            line_numbers_seen = set()
            inventories_seen = set()
            articles_by_inventory = {}
            
            with open(final_file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    summary["structure"]["total_lines"] += 1
                    
                    if not line:
                        continue
                    
                    # Analyser selon le type de ligne
                    if line.startswith('E;'):
                        summary["structure"]["header_lines_e"] += 1
                        
                    elif line.startswith('L;'):
                        summary["structure"]["header_lines_l"] += 1
                        
                    elif line.startswith('S;'):
                        self._analyze_s_line(line, line_num, summary, line_numbers_seen, 
                                           inventories_seen, articles_by_inventory)
                        
                    else:
                        summary["structure"]["invalid_lines"] += 1
                        summary["warnings"].append(f"Ligne {line_num}: Format non reconnu")
            
            # Post-traitement et calculs finaux
            self._finalize_verification_summary(summary, articles_by_inventory)
            
            # Validation finale
            self._validate_file_integrity(summary)
            
            logger.info(f"‚úÖ V√©rification fichier final termin√©e: {summary['structure']['data_lines_s']} lignes S, "
                       f"{summary['lotecart']['total_lines']} LOTECART, "
                       f"{summary['adjustments']['lines_with_adjustments']} ajustements")
            
            return summary
            
        except Exception as e:
            logger.error(f"‚ùå Erreur v√©rification fichier final: {e}", exc_info=True)
            summary["success"] = False
            summary["issues"].append(f"Erreur de v√©rification: {str(e)}")
            return summary
    
    def _analyze_s_line(self, line: str, line_num: int, summary: dict, 
                       line_numbers_seen: set, inventories_seen: set, 
                       articles_by_inventory: dict):
        """Analyse une ligne S; en d√©tail"""
        try:
            parts = line.split(';')
            summary["structure"]["data_lines_s"] += 1
            
            # V√©rifier la structure minimale
            if len(parts) < 15:
                summary["quality"]["lines_with_missing_data"] += 1
                summary["warnings"].append(f"Ligne {line_num}: Structure incompl√®te ({len(parts)} colonnes)")
                return
            
            # Extraire les donn√©es principales
            numero_session = parts[1]
            numero_inventaire = parts[2]
            rang = parts[3]
            site = parts[4]
            qte_theo_str = parts[5]
            qte_reelle_str = parts[6]
            indicateur = parts[7]
            code_article = parts[8]
            emplacement = parts[9]
            statut = parts[10]
            unite = parts[11]
            valeur_str = parts[12]
            zone_pk = parts[13]
            numero_lot = parts[14]
            
            # Validation et conversion des quantit√©s
            try:
                qte_theo = float(qte_theo_str) if qte_theo_str else 0.0
                qte_reelle = float(qte_reelle_str) if qte_reelle_str else 0.0
            except ValueError:
                summary["quality"]["lines_with_invalid_quantities"] += 1
                summary["warnings"].append(f"Ligne {line_num}: Quantit√©s invalides")
                return
            
            # V√©rifier les quantit√©s n√©gatives
            if qte_theo < 0 or qte_reelle < 0:
                summary["quantities"]["negative_quantities"] += 1
                summary["warnings"].append(f"Ligne {line_num}: Quantit√©s n√©gatives d√©tect√©es")
            
            # Accumuler les totaux
            summary["quantities"]["total_theoretical"] += qte_theo
            summary["quantities"]["total_real"] += qte_reelle
            summary["quantities"]["total_discrepancy"] += (qte_reelle - qte_theo)
            
            # Compter les z√©ros
            if qte_theo == 0:
                summary["quantities"]["zero_theoretical_count"] += 1
            if qte_reelle == 0:
                summary["quantities"]["zero_real_count"] += 1
            
            # Analyser les LOTECART
            if numero_lot == "LOTECART":
                summary["lotecart"]["total_lines"] += 1
                
                if indicateur == "2":
                    summary["lotecart"]["correct_indicators"] += 1
                else:
                    summary["lotecart"]["incorrect_indicators"] += 1
                    summary["warnings"].append(f"Ligne {line_num}: LOTECART avec indicateur incorrect ({indicateur})")
                
                # √âchantillon LOTECART
                if len(summary["lotecart"]["sample_articles"]) < 5:
                    summary["lotecart"]["sample_articles"].append({
                        "line_number": line_num,
                        "article": code_article,
                        "inventory": numero_inventaire,
                        "quantity_theo": qte_theo,
                        "quantity_real": qte_reelle,
                        "indicator": indicateur
                    })
            
            # Analyser les ajustements (indicateur = 2)
            if indicateur == "2":
                summary["adjustments"]["lines_with_indicator_2"] += 1
                
                # D√©tecter si c'est un ajustement (√©cart non nul)
                if abs(qte_reelle - qte_theo) > 0.001:
                    summary["adjustments"]["lines_with_adjustments"] += 1
                    summary["adjustments"]["articles_adjusted"].add(code_article)
                    
                    # R√©sum√© par article
                    if code_article not in summary["adjustments"]["adjustment_summary"]:
                        summary["adjustments"]["adjustment_summary"][code_article] = {
                            "total_discrepancy": 0,
                            "lines_count": 0,
                            "inventories": set()
                        }
                    
                    summary["adjustments"]["adjustment_summary"][code_article]["total_discrepancy"] += (qte_reelle - qte_theo)
                    summary["adjustments"]["adjustment_summary"][code_article]["lines_count"] += 1
                    summary["adjustments"]["adjustment_summary"][code_article]["inventories"].add(numero_inventaire)
            
            # V√©rifier les doublons de num√©ros de ligne
            try:
                rang_int = int(rang)
                if rang_int in line_numbers_seen:
                    summary["quality"]["duplicate_line_numbers"].append(rang_int)
                else:
                    line_numbers_seen.add(rang_int)
            except ValueError:
                summary["warnings"].append(f"Ligne {line_num}: Num√©ro de rang invalide ({rang})")
            
            # Tracking des inventaires
            inventories_seen.add(numero_inventaire)
            if numero_inventaire not in articles_by_inventory:
                articles_by_inventory[numero_inventaire] = set()
            articles_by_inventory[numero_inventaire].add(code_article)
            
            # √âchantillons pour les 10 premi√®res lignes S
            if len(summary["sample_data"]["first_10_s_lines"]) < 10:
                summary["sample_data"]["first_10_s_lines"].append({
                    "line_number": line_num,
                    "article": code_article,
                    "inventory": numero_inventaire,
                    "lot": numero_lot,
                    "qty_theo": qte_theo,
                    "qty_real": qte_reelle,
                    "indicator": indicateur,
                    "site": site
                })
            
            # √âchantillons d'ajustements
            if (abs(qte_reelle - qte_theo) > 0.001 and 
                len(summary["sample_data"]["adjustment_samples"]) < 5):
                summary["sample_data"]["adjustment_samples"].append({
                    "line_number": line_num,
                    "article": code_article,
                    "inventory": numero_inventaire,
                    "lot": numero_lot,
                    "qty_theo": qte_theo,
                    "qty_real": qte_reelle,
                    "discrepancy": qte_reelle - qte_theo,
                    "indicator": indicateur
                })
                
        except Exception as e:
            summary["warnings"].append(f"Ligne {line_num}: Erreur d'analyse - {str(e)}")
    
    def _finalize_verification_summary(self, summary: dict, articles_by_inventory: dict):
        """Finalise le r√©sum√© de v√©rification avec des calculs additionnels"""
        try:
            # Convertir les sets en listes pour la s√©rialisation JSON
            summary["adjustments"]["articles_adjusted"] = list(summary["adjustments"]["articles_adjusted"])
            
            # Finaliser les r√©sum√©s d'ajustements
            for article, adj_data in summary["adjustments"]["adjustment_summary"].items():
                adj_data["inventories"] = list(adj_data["inventories"])
            
            # Statistiques par inventaire
            summary["inventories"] = {}
            for inventory, articles in articles_by_inventory.items():
                summary["inventories"][inventory] = {
                    "articles_count": len(articles),
                    "articles_list": list(articles)[:10]  # Limiter √† 10 pour √©viter la surcharge
                }
            
            # Calculs de pourcentages
            total_s_lines = summary["structure"]["data_lines_s"]
            if total_s_lines > 0:
                summary["statistics"] = {
                    "lotecart_percentage": round((summary["lotecart"]["total_lines"] / total_s_lines) * 100, 2),
                    "adjustment_percentage": round((summary["adjustments"]["lines_with_adjustments"] / total_s_lines) * 100, 2),
                    "zero_theo_percentage": round((summary["quantities"]["zero_theoretical_count"] / total_s_lines) * 100, 2),
                    "indicator_2_percentage": round((summary["adjustments"]["lines_with_indicator_2"] / total_s_lines) * 100, 2)
                }
            
            # R√©sum√© global
            summary["global_summary"] = {
                "total_inventories": len(articles_by_inventory),
                "total_articles_unique": len(set().union(*[articles for articles in articles_by_inventory.values()])),
                "total_discrepancy_value": round(summary["quantities"]["total_discrepancy"], 2),
                "has_lotecart": summary["lotecart"]["total_lines"] > 0,
                "has_adjustments": summary["adjustments"]["lines_with_adjustments"] > 0
            }
            
        except Exception as e:
            summary["warnings"].append(f"Erreur finalisation r√©sum√©: {str(e)}")
    
    def _validate_file_integrity(self, summary: dict):
        """Valide l'int√©grit√© globale du fichier"""
        try:
            # V√©rifications critiques
            if summary["structure"]["data_lines_s"] == 0:
                summary["issues"].append("Aucune ligne de donn√©es S; trouv√©e")
                summary["success"] = False
            
            if summary["structure"]["header_lines_e"] == 0:
                summary["warnings"].append("Aucune ligne d'en-t√™te E; trouv√©e")
            
            if summary["structure"]["header_lines_l"] == 0:
                summary["warnings"].append("Aucune ligne d'inventaire L; trouv√©e")
            
            # V√©rifications LOTECART
            if summary["lotecart"]["total_lines"] > 0:
                if summary["lotecart"]["incorrect_indicators"] > 0:
                    summary["issues"].append(
                        f"{summary['lotecart']['incorrect_indicators']} lignes LOTECART avec indicateurs incorrects"
                    )
            
            # V√©rifications de coh√©rence
            if len(summary["quality"]["duplicate_line_numbers"]) > 0:
                summary["issues"].append(
                    f"Num√©ros de ligne dupliqu√©s: {summary['quality']['duplicate_line_numbers'][:10]}"
                )
            
            # V√©rifications des quantit√©s
            if summary["quantities"]["negative_quantities"] > 0:
                summary["warnings"].append(
                    f"{summary['quantities']['negative_quantities']} lignes avec quantit√©s n√©gatives"
                )
            
            # Score de qualit√© (0-100)
            quality_score = 100
            quality_score -= min(len(summary["issues"]) * 20, 80)  # -20 par issue critique
            quality_score -= min(len(summary["warnings"]) * 5, 20)  # -5 par warning
            quality_score -= min(summary["quality"]["lines_with_invalid_quantities"] * 2, 10)
            
            summary["quality_score"] = max(quality_score, 0)
            
            # D√©terminer le succ√®s global
            if len(summary["issues"]) > 0:
                summary["success"] = False
            
        except Exception as e:
            summary["issues"].append(f"Erreur validation int√©grit√©: {str(e)}")
            summary["success"] = False
    
    def _verify_quantities_consistency(self, final_file_path: str, completed_df: pd.DataFrame, distributed_df: pd.DataFrame) -> dict:
        """V√©rifie la coh√©rence entre quantit√©s th√©oriques ajust√©es et quantit√©s r√©elles"""
        verification = {
            "success": True,
            "total_lines_checked": 0,
            "consistent_lines": 0,
            "inconsistent_lines": 0,
            "lotecart_lines": 0,
            "adjusted_lines": 0,
            "standard_lines": 0,
            "issues": [],
            "samples": {
                "consistent": [],
                "inconsistent": [],
                "lotecart": [],
                "adjusted": []
            }
        }
        
        try:
            # Cr√©er les dictionnaires de r√©f√©rence
            completed_dict = {}
            for _, row in completed_df.iterrows():
                key = (row["Code Article"], row["Num√©ro Inventaire"], str(row["Num√©ro Lot"]).strip())
                completed_dict[key] = {
                    "qte_theo_originale": row["Quantit√© Th√©orique"],
                    "qte_reelle_saisie": row["Quantit√© R√©elle"]
                }
            
            adjustments_dict = {}
            for _, row in distributed_df.iterrows():
                key = (row["CODE_ARTICLE"], row["NUMERO_INVENTAIRE"], str(row["NUMERO_LOT"]).strip())
                adjustments_dict[key] = {
                    "qte_theo_ajustee": row["QUANTITE_CORRIGEE"],
                    "type_lot": row["TYPE_LOT"],
                    "ajustement": row["AJUSTEMENT"]
                }
            
            # Analyser le fichier final
            with open(final_file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    if line.startswith('S;'):
                        parts = line.strip().split(';')
                        if len(parts) >= 15:
                            verification["total_lines_checked"] += 1
                            
                            # Extraire les donn√©es
                            code_article = parts[8]
                            numero_inventaire = parts[2]
                            numero_lot = parts[14].strip()
                            qte_theo_finale = float(parts[5]) if parts[5] else 0
                            qte_reelle_finale = float(parts[6]) if parts[6] else 0
                            
                            key = (code_article, numero_inventaire, numero_lot)
                            
                            # R√©cup√©rer les donn√©es de r√©f√©rence
                            completed_data = completed_dict.get(key, {})
                            adjustment_data = adjustments_dict.get(key, {})
                            
                            # D√©terminer le type de ligne
                            line_type = "standard"
                            expected_qte_theo = completed_data.get("qte_theo_originale", 0)
                            expected_qte_reelle = completed_data.get("qte_reelle_saisie", 0)
                            
                            if numero_lot == "LOTECART":
                                line_type = "lotecart"
                                verification["lotecart_lines"] += 1
                                # Pour LOTECART : qt√© th√©o = qt√© r√©elle
                                expected_qte_theo = expected_qte_reelle
                            elif key in adjustments_dict:
                                line_type = "adjusted"
                                verification["adjusted_lines"] += 1
                                # Pour ajustements : qt√© th√©o ajust√©e
                                expected_qte_theo = adjustment_data["qte_theo_ajustee"]
                            else:
                                verification["standard_lines"] += 1
                            
                            # V√©rifier la coh√©rence
                            theo_ok = abs(qte_theo_finale - expected_qte_theo) < 0.001
                            reelle_ok = abs(qte_reelle_finale - expected_qte_reelle) < 0.001
                            
                            if theo_ok and reelle_ok:
                                verification["consistent_lines"] += 1
                                
                                # √âchantillon de lignes coh√©rentes
                                if len(verification["samples"]["consistent"]) < 3:
                                    verification["samples"]["consistent"].append({
                                        "line": line_num,
                                        "article": code_article,
                                        "lot": numero_lot,
                                        "type": line_type,
                                        "qte_theo": qte_theo_finale,
                                        "qte_reelle": qte_reelle_finale
                                    })
                            else:
                                verification["inconsistent_lines"] += 1
                                
                                # D√©tail de l'incoh√©rence
                                issue = {
                                    "line": line_num,
                                    "article": code_article,
                                    "lot": numero_lot,
                                    "type": line_type,
                                    "qte_theo_attendue": expected_qte_theo,
                                    "qte_theo_trouvee": qte_theo_finale,
                                    "qte_reelle_attendue": expected_qte_reelle,
                                    "qte_reelle_trouvee": qte_reelle_finale,
                                    "theo_ok": theo_ok,
                                    "reelle_ok": reelle_ok
                                }
                                
                                verification["issues"].append(
                                    f"Ligne {line_num} ({code_article}): "
                                    f"Th√©o attendue={expected_qte_theo}, trouv√©e={qte_theo_finale}, "
                                    f"R√©elle attendue={expected_qte_reelle}, trouv√©e={qte_reelle_finale}"
                                )
                                
                                # √âchantillon d'incoh√©rences
                                if len(verification["samples"]["inconsistent"]) < 5:
                                    verification["samples"]["inconsistent"].append(issue)
                            
                            # √âchantillons par type
                            if line_type == "lotecart" and len(verification["samples"]["lotecart"]) < 3:
                                verification["samples"]["lotecart"].append({
                                    "line": line_num,
                                    "article": code_article,
                                    "qte_theo": qte_theo_finale,
                                    "qte_reelle": qte_reelle_finale,
                                    "consistent": theo_ok and reelle_ok
                                })
                            elif line_type == "adjusted" and len(verification["samples"]["adjusted"]) < 3:
                                verification["samples"]["adjusted"].append({
                                    "line": line_num,
                                    "article": code_article,
                                    "lot": numero_lot,
                                    "qte_theo_originale": completed_data.get("qte_theo_originale", 0),
                                    "qte_theo_ajustee": qte_theo_finale,
                                    "qte_reelle": qte_reelle_finale,
                                    "ajustement": adjustment_data.get("ajustement", 0),
                                    "consistent": theo_ok and reelle_ok
                                })
            
            # Calculs finaux
            if verification["total_lines_checked"] > 0:
                consistency_rate = (verification["consistent_lines"] / verification["total_lines_checked"]) * 100
                verification["consistency_rate"] = round(consistency_rate, 2)
                
                if consistency_rate < 95:
                    verification["success"] = False
                    verification["issues"].insert(0, f"Taux de coh√©rence trop faible: {consistency_rate}%")
            
            # R√©sum√©
            verification["summary"] = {
                "total_checked": verification["total_lines_checked"],
                "consistent": verification["consistent_lines"],
                "inconsistent": verification["inconsistent_lines"],
                "consistency_rate": verification.get("consistency_rate", 0),
                "lotecart_count": verification["lotecart_lines"],
                "adjusted_count": verification["adjusted_lines"],
                "standard_count": verification["standard_lines"]
            }
            
            logger.info(
                f"‚úÖ V√©rification quantit√©s termin√©e: {verification['consistent_lines']}/{verification['total_lines_checked']} "
                f"lignes coh√©rentes ({verification.get('consistency_rate', 0)}%)"
            )
            
            return verification
            
        except Exception as e:
            logger.error(f"‚ùå Erreur v√©rification quantit√©s: {e}", exc_info=True)
            verification["success"] = False
            verification["issues"].append(f"Erreur de v√©rification: {str(e)}")
            return verification


# Initialisation du processeur
processor = SageX3Processor()


# Endpoints API
@app.route("/api/upload", methods=["POST"])
@apply_rate_limit("upload")
@handle_api_errors("file_upload")
def upload_file():
    """Endpoint am√©lior√© pour l'upload initial d'un fichier Sage X3"""
    if "file" not in request.files:
        return jsonify({"error": "Aucun fichier fourni"}), 400

    file = request.files["file"]
    if not file.filename:
        return jsonify({"error": "Nom de fichier vide"}), 400

    file_extension = os.path.splitext(file.filename)[1].lower()
    if file_extension not in [".csv", ".xlsx", ".xls"]:
        return (
            jsonify({"error": "Format non support√©. Seuls CSV et XLSX sont accept√©s"}),
            400,
        )

    # Validation s√©curis√©e du fichier
    is_valid, error_msg = FileValidator.validate_file_security(
        file, config.MAX_FILE_SIZE
    )
    if not is_valid:
        return jsonify({"error": error_msg}), 400

    session_creation_timestamp = datetime.now()
    filepath = None

    try:
        # Cr√©er la session en base de donn√©es
        session_id = session_service.create_session(
            original_filename=file.filename,
            original_file_path="",  # Sera mis √† jour apr√®s sauvegarde
            status="uploading",
        )

        filename_on_disk = secure_filename(f"{session_id}_{file.filename}")
        filepath = os.path.join(config.UPLOAD_FOLDER, filename_on_disk)
        file.save(filepath)

        # Mettre √† jour le chemin du fichier
        session_service.update_session(session_id, original_file_path=filepath)

        # Traitement du fichier
        is_valid, result_data, headers, inventory_date = (
            file_processor.validate_and_process_sage_file(
                filepath, file_extension, session_creation_timestamp
            )
        )

        if not is_valid:
            if os.path.exists(filepath):
                os.remove(filepath)
            session_service.delete_session(session_id)
            return jsonify({"error": str(result_data)}), 400

        original_df = result_data

        # Agr√©gation
        aggregated_df = file_processor.aggregate_data(original_df)

        # G√©n√©ration du template
        template_file_path = file_processor.generate_template(
            aggregated_df, session_id, config.PROCESSED_FOLDER
        )

        # Mise √† jour de la session
        session_service.update_session(
            session_id,
            template_file_path=template_file_path,
            status="template_generated",
            inventory_date=inventory_date,
            nb_articles=len(aggregated_df),
            nb_lots=len(original_df),
            total_quantity=float(aggregated_df["Quantite_Theorique_Totale"].sum()),
            header_lines=json.dumps(headers),
        )

        # Sauvegarder les DataFrames de mani√®re persistante
        session_service.save_dataframe(session_id, "original_df", original_df)
        session_service.save_dataframe(session_id, "aggregated_df", aggregated_df)

        # Sauvegarder aussi dans le stockage persistant
        session_service.save_dataframe(session_id, "original_df", original_df)
        session_service.save_dataframe(session_id, "aggregated_df", aggregated_df)

        return jsonify(
            {
                "success": True,
                "session_id": session_id,
                "template_url": f"/api/download/template/{session_id}",
                "stats": {
                    "nb_articles": len(aggregated_df),
                    "total_quantity": float(
                        aggregated_df["Quantite_Theorique_Totale"].sum()
                    ),
                    "nb_lots": len(original_df),
                    "inventory_date": (
                        inventory_date.isoformat() if inventory_date else None
                    ),
                },
            }
        )

    except Exception as e:
        logger.error(f"Erreur upload: {e}", exc_info=True)
        if filepath and os.path.exists(filepath):
            os.remove(filepath)
        return jsonify({"error": "Erreur interne du serveur"}), 500


@app.route("/api/process", methods=["POST"])
@apply_rate_limit("upload")
@handle_api_errors("file_processing")
def process_completed_file_route():
    """Endpoint am√©lior√© pour traiter le fichier compl√©t√©"""
    if "file" not in request.files or "session_id" not in request.form:
        return jsonify({"error": "Param√®tres manquants"}), 400

    try:
        session_id = request.form["session_id"]
        file = request.files["file"]
        strategy = request.form.get("strategy", "FIFO")

        # V√©rifier que la session existe
        session_data = session_service.get_session_data(session_id)
        if not session_data:
            return jsonify({"error": "Session non trouv√©e"}), 404

        if not file.filename.lower().endswith((".xlsx", ".xls")):
            return jsonify({"error": "Seuls les fichiers Excel sont accept√©s"}), 400

        # Validation du fichier compl√©t√©
        temp_filepath = os.path.join(
            config.PROCESSED_FOLDER, f"temp_{session_id}_{file.filename}"
        )
        file.save(temp_filepath)

        is_valid, validation_msg, errors = file_processor.validate_completed_template(
            temp_filepath
        )
        if not is_valid:
            os.remove(temp_filepath)
            return jsonify({"error": validation_msg, "details": errors}), 400

        filename_on_disk = secure_filename(f"completed_{session_id}_{file.filename}")
        filepath = os.path.join(config.PROCESSED_FOLDER, filename_on_disk)
        os.rename(temp_filepath, filepath)

        # Traitement (utilise encore l'ancienne m√©thode pour compatibilit√©)
        processed_summary_df = processor.process_completed_file(session_id, filepath)
        distributed_summary_df = processor.distribute_discrepancies(
            session_id, strategy
        )
        final_file_path = processor.generate_final_file(session_id)

        # Mise √† jour de la session en base
        session_service.update_session(
            session_id,
            completed_file_path=filepath,
            final_file_path=final_file_path,
            status="completed",
            strategy_used=strategy,
        )

        # R√©cup√©rer les donn√©es depuis les services
        session_data = session_service.get_session_data(session_id)
        if not session_data:
            return jsonify({"error": "Session non trouv√©e"}), 404

        return jsonify(
            {
                "success": True,
                "final_url": f"/api/download/final/{session_id}",
                "stats": {
                    "total_discrepancy": session_data.get("total_discrepancy", 0),
                    "adjusted_items": session_data.get("adjusted_items_count", 0),
                    "strategy_used": session_data.get("strategy_used", "N/A"),
                },
            }
        )

    except ValueError as e:
        logger.error(f"Erreur validation: {e}")
        return jsonify({"error": str(e)}), 400
    except Exception as e:
        logger.error(f"Erreur traitement: {e}", exc_info=True)
        return jsonify({"error": "Erreur interne du serveur"}), 500


@app.route("/api/download/<file_type>/<session_id>", methods=["GET"])
@handle_api_errors("file_download")
def download_file(file_type: str, session_id: str):
    """Endpoint de t√©l√©chargement am√©lior√©"""
    try:
        session_data = session_service.get_session_data(session_id)
        if not session_data:
            return jsonify({"error": "Session non trouv√©e"}), 404

        filepath = None
        download_name = None
        mimetype = None

        if file_type == "template":
            filepath = session_data["template_file_path"]
            if not filepath:
                return jsonify({"error": "Template non g√©n√©r√©"}), 404
            download_name = os.path.basename(filepath)
            mimetype = (
                "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
        elif file_type == "final":
            filepath = session_data["final_file_path"]
            if not filepath:
                return jsonify({"error": "Fichier final non g√©n√©r√©"}), 404
            download_name = os.path.basename(filepath)
            mimetype = "text/csv"
        else:
            return jsonify({"error": "Type de fichier invalide"}), 400

        if not os.path.exists(filepath):
            return jsonify({"error": "Fichier non trouv√© sur le serveur"}), 404

        return send_file(
            filepath, as_attachment=True, download_name=download_name, mimetype=mimetype
        )

    except Exception as e:
        logger.error(f"Erreur t√©l√©chargement: {e}", exc_info=True)
        return jsonify({"error": "Erreur interne du serveur"}), 500


@app.route("/api/sessions", methods=["GET"])
def list_sessions():
    """Liste les sessions avec pagination"""
    try:
        limit = int(request.args.get("limit", 50))
        include_expired = request.args.get("include_expired", "false").lower() == "true"

        sessions_list = session_service.list_sessions(
            limit=limit, include_expired=include_expired
        )

        return jsonify({"sessions": sessions_list})

    except Exception as e:
        logger.error(f"Erreur listage sessions: {e}", exc_info=True)
        return jsonify({"error": "Erreur interne du serveur"}), 500


@app.route("/api/sessions/<session_id>", methods=["DELETE"])
def delete_session(session_id: str):
    """Supprime une session"""
    try:
        success = session_service.delete_session(session_id)
        if success:
            # Nettoyer les donn√©es de session
            session_service.cleanup_session_data(session_id)
            return jsonify({"success": True})
        else:
            return jsonify({"error": "Session non trouv√©e"}), 404
    except Exception as e:
        logger.error(f"Erreur suppression session: {e}")
        return jsonify({"error": "Erreur interne du serveur"}), 500


@app.route("/api/analyze/<session_id>", methods=["GET"])
def analyze_file_format(session_id: str):
    """Endpoint pour analyser le format d'un fichier upload√©"""
    try:
        session = session_service.get_session(session_id)
        if not session:
            return jsonify({"error": "Session non trouv√©e"}), 404

        filepath = session.original_file_path
        if not os.path.exists(filepath):
            return jsonify({"error": "Fichier non trouv√©"}), 404

        format_detected, format_msg, format_info = file_processor.detect_file_format(
            filepath
        )

        return jsonify(
            {
                "success": format_detected,
                "message": format_msg,
                "format_info": format_info,
                "expected_format": {
                    "columns_required": len(file_processor.SAGE_COLUMN_NAMES_ORDERED),
                    "column_names": file_processor.SAGE_COLUMN_NAMES_ORDERED,
                    "expected_line_types": ["E", "L", "S"],
                },
            }
        )

    except Exception as e:
        logger.error(f"Erreur analyse format: {e}")
        return jsonify({"error": "Erreur interne du serveur"}), 500


@app.route("/api/health", methods=["GET"])
def health_check():
    """Endpoint de sant√© am√©lior√©"""
    try:
        db_healthy = db_manager.health_check()
        sessions_count = len(session_service.list_sessions(limit=1000))

        status = "healthy" if db_healthy else "degraded"

        return jsonify(
            {
                "status": status,
                "timestamp": datetime.now().isoformat(),
                "database": "healthy" if db_healthy else "error",
                "active_sessions_count": sessions_count,
            }
        )
    except Exception as e:
        logger.error(f"Health check error: {e}")
        return (
            jsonify(
                {
                    "status": "error",
                    "timestamp": datetime.now().isoformat(),
                    "error": str(e),
                }
            ),
            500,
        )


# T√¢che de nettoyage (√† ex√©cuter p√©riodiquement)
@app.route("/api/cleanup", methods=["POST"])
def cleanup_sessions():
    """Nettoie les sessions expir√©es"""
    try:
        hours = int(request.json.get("hours", 24))

        # Nettoyage des sessions en base
        cleaned_sessions = session_service.cleanup_expired_sessions(hours)

        # Nettoyage des fichiers anciens
        days_old = int(request.json.get("days_old", 7))
        file_stats = file_manager.cleanup_old_files(days_old)

        return jsonify(
            {
                "cleaned_sessions": cleaned_sessions,
                "cleaned_files": file_stats,
                "total_files_cleaned": sum(file_stats.values()),
            }
        )
    except Exception as e:
        logger.error(f"Erreur nettoyage: {e}")
        return jsonify({"error": "Erreur nettoyage"}), 500


@app.route("/api/archive/<session_id>", methods=["POST"])
def archive_session(session_id: str):
    """Archive une session et ses fichiers"""
    try:
        # V√©rifier que la session existe
        session_data = session_service.get_session_data(session_id)
        if not session_data:
            return jsonify({"error": "Session non trouv√©e"}), 404

        # Archiver les fichiers
        success = file_manager.archive_session_files(
            session_id, session_data.get("created_at")
        )

        if success:
            # Marquer la session comme archiv√©e
            session_service.update_session(session_id, status="archived")
            return jsonify({"success": True, "message": "Session archiv√©e avec succ√®s"})
        else:
            return jsonify({"error": "Erreur lors de l'archivage"}), 500

    except Exception as e:
        logger.error(f"Erreur archivage session {session_id}: {e}")
        return jsonify({"error": "Erreur interne du serveur"}), 500


@app.route("/api/verify/<session_id>", methods=["GET"])
@handle_api_errors("file_verification")
def verify_final_file(session_id: str):
    """Endpoint pour v√©rifier en d√©tail le fichier final d'une session"""
    try:
        # R√©cup√©rer les donn√©es de session
        session_data = session_service.get_session_data(session_id)
        if not session_data:
            return jsonify({"error": "Session non trouv√©e"}), 404
        
        final_file_path = session_data.get("final_file_path")
        if not final_file_path:
            return jsonify({"error": "Aucun fichier final g√©n√©r√© pour cette session"}), 404
        
        # Effectuer la v√©rification d√©taill√©e
        verification_summary = processor._verify_final_file_with_summary(final_file_path)
        
        # Ajouter des m√©tadonn√©es de session
        verification_summary["session_info"] = {
            "session_id": session_id,
            "original_filename": session_data.get("original_filename"),
            "status": session_data.get("status"),
            "created_at": session_data.get("created_at"),
            "strategy_used": session_data.get("strategy_used", "FIFO")
        }
        
        return jsonify({
            "success": True,
            "verification": verification_summary
        })
        
    except Exception as e:
        logger.error(f"Erreur v√©rification session {session_id}: {e}")
        return jsonify({"error": "Erreur lors de la v√©rification"}), 500

@app.route("/api/verify-quantities/<session_id>", methods=["GET"])
@handle_api_errors("quantities_verification")
def verify_quantities_consistency(session_id: str):
    """Endpoint pour v√©rifier sp√©cifiquement la coh√©rence des quantit√©s"""
    try:
        # R√©cup√©rer les donn√©es de session
        session_data = session_service.get_session_data(session_id)
        if not session_data:
            return jsonify({"error": "Session non trouv√©e"}), 404
        
        final_file_path = session_data.get("final_file_path")
        if not final_file_path:
            return jsonify({"error": "Aucun fichier final g√©n√©r√© pour cette session"}), 404
        
        # Charger les DataFrames n√©cessaires
        completed_df = session_service.load_dataframe(session_id, "completed_df")
        distributed_df = session_service.load_dataframe(session_id, "distributed_df")
        
        if completed_df is None or distributed_df is None:
            return jsonify({"error": "Donn√©es de traitement manquantes"}), 404
        
        # Effectuer la v√©rification des quantit√©s
        quantities_verification = processor._verify_quantities_consistency(
            final_file_path, completed_df, distributed_df
        )
        
        return jsonify({
            "success": True,
            "session_id": session_id,
            "quantities_verification": quantities_verification,
            "message": "V√©rification des quantit√©s termin√©e"
        })
        
    except Exception as e:
        logger.error(f"Erreur v√©rification quantit√©s session {session_id}: {e}")
        return jsonify({"error": "Erreur lors de la v√©rification des quantit√©s"}), 500

@app.route("/api/stats/sessions", methods=["GET"])
@handle_api_errors("session_stats")
def get_session_stats():
    """Retourne les statistiques d√©taill√©es de toutes les sessions"""
    try:
        # R√©cup√©rer toutes les sessions
        sessions = session_service.list_sessions(limit=100, include_expired=True)
        
        # Calculer les statistiques globales
        stats = {
            "total_sessions": len(sessions),
            "sessions_by_status": {},
            "total_files_processed": 0,
            "total_articles_processed": 0,
            "total_quantity_processed": 0,
            "total_discrepancies": 0,
            "sessions_with_lotecart": 0,
            "strategies_used": {},
            "recent_sessions": [],
            "processing_times": {
                "last_24h": 0,
                "last_7d": 0,
                "last_30d": 0
            }
        }
        
        from datetime import datetime, timedelta
        now = datetime.utcnow()
        
        for session in sessions:
            # Compter par statut
            status = session.get("status", "unknown")
            stats["sessions_by_status"][status] = stats["sessions_by_status"].get(status, 0) + 1
            
            # Accumuler les totaux
            session_stats = session.get("stats", {})
            stats["total_articles_processed"] += session_stats.get("nb_articles", 0)
            stats["total_quantity_processed"] += session_stats.get("total_quantity", 0)
            stats["total_discrepancies"] += session_stats.get("total_discrepancy", 0)
            
            # Compter les strat√©gies
            strategy = session_stats.get("strategy_used", "unknown")
            stats["strategies_used"][strategy] = stats["strategies_used"].get(strategy, 0) + 1
            
            # Sessions r√©centes (derni√®res 10)
            if len(stats["recent_sessions"]) < 10:
                stats["recent_sessions"].append({
                    "id": session["id"],
                    "filename": session.get("original_filename", "N/A"),
                    "status": status,
                    "created_at": session.get("created_at"),
                    "articles": session_stats.get("nb_articles", 0),
                    "discrepancy": session_stats.get("total_discrepancy", 0)
                })
            
            # Compter par p√©riode
            if session.get("created_at"):
                try:
                    created_at = datetime.fromisoformat(session["created_at"].replace('Z', '+00:00'))
                    if created_at > now - timedelta(hours=24):
                        stats["processing_times"]["last_24h"] += 1
                    if created_at > now - timedelta(days=7):
                        stats["processing_times"]["last_7d"] += 1
                    if created_at > now - timedelta(days=30):
                        stats["processing_times"]["last_30d"] += 1
                except:
                    pass
        
        return jsonify({
            "success": True,
            "stats": stats,
            "generated_at": now.isoformat()
        })
        
    except Exception as e:
        logger.error(f"Erreur r√©cup√©ration stats sessions: {e}")
        return jsonify({"error": "Erreur interne du serveur"}), 500

@app.route("/api/stats/files", methods=["GET"])
def get_file_stats():
    """Retourne les statistiques des fichiers"""
    try:
        stats = file_manager.get_folder_stats()
        return jsonify({"folder_stats": stats})
    except Exception as e:
        logger.error(f"Erreur stats fichiers: {e}")
        return jsonify({"error": "Erreur interne du serveur"}), 500


if __name__ == "__main__":
    logger.info("D√©marrage de l'application Moulinette Sage X3")
    app.run(host="0.0.0.0", port=5000, debug=True)
